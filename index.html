<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="cniabksS1E02e_pI71dMejZJKNxNqszUIJ--H3F0p2g" />
        <link rel="stylesheet" href="style.css">
        <link rel="stylesheet" href="post.css">
        <title>Disarray</title>

    </head>
    
    <body>
    
        <div id="root-folder" value="."></div>
        <div id="header"></div>
        
        <h1>
            <img id="theme-icon" width=20px height=20px src="icons/spark.svg"/> Welcome to Disarray
        </h1>

        <p>
            Hello and welcome to my blog! As a postdoctoral researcher in machine learning and artificial intelligence, I’m here to share my journey, insights, and notes with you. I strive to be as clear and grounded as possible, and I hope you’ll find something interesting and valuable in my posts.
        </p>

        <div onclick="location.href='posts/02-09-25-torch-distributed/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    <img src="icons/pin.svg" width=16px height=16px/> A Torch Distributed Tutorial
                </div>
                <div class="post-desc"> 
                    In this post, I will provide a simple tutorial on how to use PyTorch's distributed package. We will cover the basics of setting up a distributed environment, initializing the process group and up until a multi-node multi-GPU inference example. Bear in mind that I am no expert on this topic, so if you find any mistakes or have suggestions for improvement, please let me know!
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 02 Septempber, 2025  |  Estimated Reading Time: ~30 min 
                </div>
            </div>
        </article>
        </div>
        <br>

        <div onclick="location.href='posts/06-03-25-tut/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    <img src="icons/pin.svg" width=16px height=16px/> Mid-Training Untying: A Fix That Barely Fixes Anything 
                </div>
                <div class="post-desc"> 
                    In one of the previous posts, we have seen how tying embeddings can be destabilize the training if the the data do not satisfy certain assumptions (see <a href="https://openreview.net/forum?id=yyYMAprcAR">here</a>). In this post, we will explore a simple idea to get the best of both worlds: early training boost with tied embeddings and late training stability with untied one. This was a research idea that I had in mind however it did not work as well as expected so I decided to share it here. 
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 06 March, 2025  |  Estimated Reading Time: ~5 min 
                </div>
            </div>
        </article>
        </div>
        <br>

        <div onclick="location.href='posts/29-12-24-chaos-II/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    <img src="icons/pin.svg" width=16px height=16px/> Chaos Theory with Differential Topology (Part II).
                </div>
                <div class="post-desc"> 
                    This is the second part of our series on chaos theory. In this post, we will introduce the concept of differentiable manifolds and tangent spaces. Using these, we will explore the differentiation of maps between these objects. Finally, we will bring everything together with a simple example.
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 29 December, 2024  |  Estimated Reading Time: ~20 min 
                </div>
            </div>
        </article>
        </div>
        <br>

        <div onclick="location.href='posts/15-12-24-chaos-I/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    <img src="icons/pin.svg" width=16px height=16px/> Chaos Theory with Differential Topology (Part I).
                </div>
                <div class="post-desc"> 
                    A brief introduction to chaos theory from a differential topolody perspecitive from a guy that is studying these topics for fun. I will share some of the things I learned from Alligood et al. (1998). This is also an attempt to understand the basics of differential topology. Here we discuss basics of differential topology and basic definitions of dynamical systems.
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 15 December, 2024  |  Estimated Reading Time: ~20-30 min 
                </div>
            </div>
        </article>
        </div>
        <br>

        <div onclick="location.href='posts/26-09-24-icml-story/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    <img src="icons/pin.svg" width=16px height=16px/> Semantics of LLM, Weight Tying, and a story.
                </div>
                <div class="post-desc"> 
                    This summer, I was lucky enough to being accepted to ICML 2024 as spotlight poster. I told this story on reddit (you can see the original post <a href=https://www.reddit.com/r/MachineLearning/comments/1eqm0lr/r_why_and_when_tying_embedding_a_story/>here</a>). However, now that I have a blog, I thought it would be nice to keep it here as well. Well, enough with the introduction, let's get to the story behind the <a href="https://icml.cc/virtual/2024/poster/32648">paper</a> titled <b>By tying embedding you are assuming the distributional hypothesis</b>.
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 26 September, 2024  |  Estimated Reading Time: ~11 min 
                </div>
            </div>
        </article>
        </div>
        <br>

        <div onclick="location.href='posts/19-09-24-phase/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    Superposition, Phase Diagrams, and Regularization.
                </div>
                <div class="post-desc"> 
                    For anyone who read through the <a href=https://transformer-circuits.pub/2022/toy_model/index.html#phase-change>toy models of superposition</a>. This is mostly a rehash of the same ideas presented in the third section of Elhage et al. 's work. However, to keep things new, I will introduce a regularization term to the loss function. This will allow us to explore the phase diagram in a novel scenario. This analysis suggests that regularization inhibits superposition.
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 19 September, 2024  |  Estimated Reading Time: ~15 min 
                </div>
            </div>
        </article>
        </div>
        <br>



        <div onclick="location.href='posts/06-09-24-tfree/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    T-Free: Sparse Embedding Representations.
                </div>
                <div class="post-desc"> 
                    In this post, I will discuss T-Free. A recent paper from <a href="https://aleph-alpha.com/">aleph-alpha</a> that introduces a new method for learning embedding representations that promises to be both memory and computationally efficient. Of course this post is based from their original <a href=https://arxiv.org/abs/2406.19223>paper</a>. However, I will share some of my insights and thoughts on their work.
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 6 September, 2024  |  Estimated Reading Time: ~15 min 
                </div>
            </div>
        </article>
        </div>
        <br>

        <div onclick="location.href='posts/28-08-24-cce-minimizer/index.html';" style="cursor: pointer;">
        <article class="light-post-theme post-block"> 
            <div class="post-inner">
                <div class="post-title"> 
                    Population Minimizer of The Categorical Cross Entropy Loss 
                </div>
                <div class="post-desc"> 
                    In this post, I will explore how to find the population minimizer for the conditional risk associated with Categorical Cross Entropy. We will use only basic fundamental probability principles in order to make the derivation accessible to anyone with a basic understanding of probability. The only analytical tool needed are the Lagrange multipliers for constrained optimization problems.
                </div>
                <div class="post-foot"> 
                    <img src="icons/clock.svg" width=12px height=12px/> Date: 29 August, 2024  |  Estimated Reading Time: 10-30 min 
                </div>
            </div>
        </article>
        </div>

        <div id="footer"></div>

        <script src="scripts/toggle-theme.js"></script>
        <script src="scripts/load-header.js"></script>
        <script src="scripts/load-footer.js"></script>
    </body>

</html>
